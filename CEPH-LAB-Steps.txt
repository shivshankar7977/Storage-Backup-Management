## Nodes Setup

1. Create 5 nodes with 1 GB RAM and 1 core CPU each. Let's denote them as follows:

    * ceph-controller
    * ceph-compute01
    * ceph-compute02
    * ceph-monitor
    * ceph-client

2. Attach 3 disks of 20 GB each to `ceph-compute01` and `ceph-compute02`.

## Hostnames Configuration (Execute these commands on the respective nodes)

3. On ceph-controller:
    
    hostnamectl set-hostname ceph-controller
    

4. On ceph-compute01:
    
    hostnamectl set-hostname ceph-compute01
    

5. On ceph-compute02:
    
    hostnamectl set-hostname ceph-compute02
    

6. On ceph-monitor:
    
    hostnamectl set-hostname ceph-monitor
    

7. On ceph-client:
    
    hostnamectl set-hostname ceph-client
    

## Create FQDN Entry

8. Create FQDN Entry of hostname on all nodes with their respective IP:

    
    echo '192.168.xxx.xxx  ceph-controller.hpcsa.cdac.in  ceph-controller' >> /etc/hosts
    echo '192.168.xxx.xxx  ceph-compute01.hpcsa.cdac.in   ceph-compute01' >> /etc/hosts
    echo '192.168.xxx.xxx  ceph-compute02.hpcsa.cdac.in   ceph-compute02' >> /etc/hosts
    echo '192.168.xxx.xxx  ceph-monitor.hpcsa.cdac.in     ceph-monitor' >> /etc/hosts
    echo '192.168.xxx.xxx  ceph-client.hpcsa.cdac.in      ceph-client' >> /etc/hosts
    
   Replace `192.168.xxx.xxx` with the respective IP addresses.

## Basic Node Configuration (Execute these commands on all nodes)

9. Disable Firewall:
    
    systemctl stop firewalld && systemctl disable firewalld
    

10. Install chrony for time synchronization:
    
    yum install chrony -y
    chronyc sourcestats
    

11. Create a user for Ceph deployment and disable its password:
    
    useradd cephadm && echo "cdac" | passwd --stdin cephadm
    

12. Allow cephadm user to run sudo commands without a password:
    
    echo "cephadm ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephadm
    chmod 0440 /etc/sudoers.d/cephadm
    

13. Disable SELinux:
    
    sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
    

14. Install the EPEL repository:
    
    sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
    

15. Reboot each node:
    
    reboot
    

## Chrony Configuration (Execute these commands on all nodes except ceph-controller)

16. Modify the chrony configuration:
    
   vi /etc/chrony.conf
    
    Comment all available servers and add the line `server ceph-controller`.

17. Restart chrony:
    
    systemctl restart chronyd
    

## Ceph Controller Configuration

18. Install the Ceph release package and Ceph deployment tools:
    
    sudo rpm -Uvh https://download.ceph.com/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm
    yum update -y &&  yum install ceph-deploy python2-pip  -y
    

19. Generate SSH keys for the cephadm user:
    
    su - cephadm
    ssh-keygen
    
    Hit enter to accept the defaults.

20. Copy the SSH keys to the other nodes:
    
    ssh-copy-id cephadm@ceph-compute01
    ssh-copy-id cephadm@ceph-compute02
    ssh-copy-id cephadm@ceph-monitor
    ssh-copy-id cephadm@ceph-client
    

21. Create an SSH configuration file to specify the username to use when SSHing to the other nodes:
    
    vi ~/.ssh/config
    
    Add the following lines:
    
    Host ceph-compute01
       Hostname ceph-compute01
       User cephadm
    Host ceph-compute02
       Hostname ceph-compute02
       User cephadm
    Host ceph-monitor
       Hostname ceph-monitor
       User cephadm
    Host ceph-client
       Hostname ceph-client
       User cephadm
    
22. Adjust the permissions on the SSH configuration file:
    
    chmod 644 ~/.ssh/config
    
23. Test the SSH configuration:
    
    ssh cephadm@ceph-monitor
    ssh cephadm@ceph-compute01
    ssh cephadm@ceph-compute02
    

## Ceph Cluster Configuration (Execute these commands as the cephadm user on the Ceph controller)

24. Create a new directory for the Ceph cluster and navigate into it:
    
    mkdir ceph_cluster
    cd ceph_cluster
    

25. Create a new cluster:
    
    ceph-deploy new monitor
    

26. Edit the Ceph configuration file:
    
    vi ceph.conf
    
    Add the line `public network = 192.168.xxx.xxx/24`, replace `192.168.xxx.xxx` with your local network.

Please note, you will need to replace placeholders like `192.168.xxx.xxx` with your actual values.

27. Install Ceph on the nodes:
    
    ceph-deploy install controller compute0 compute1 monitor
    sudo ceph --version
    
  

## Initialize the Monitor and Create the Ceph Storage Cluster

28. Initialize the monitor:
    
    ceph-deploy mon create-initial
    

29. Deploy the Ceph client admin keyring:
    
    ceph-deploy admin controller compute0 compute1 monitor
    

30. Create Ceph manager daemons:
    
    ceph-deploy mgr create compute0 compute1
    

31. List the disks available:
    
    ceph-deploy disk list compute0 compute1
    

32. Prepare and activate the OSDs (Replace `/dev/sdb`, `/dev/sdc`, and `/dev/sdd` with the appropriate disk names):
    
    ceph-deploy osd create --data /dev/sdb compute0
    ceph-deploy osd create --data /dev/sdc compute0
    ceph-deploy osd create --data /dev/sdd compute0

    ceph-deploy osd create --data /dev/sdb compute1
    ceph-deploy osd create --data /dev/sdc compute1
    ceph-deploy osd create --data /dev/sdd compute1



33. Install Ceph on the client and distribute the admin keyring:
    
    ceph-deploy install client
    ceph-deploy admin client
    

## Verify the Ceph Storage Cluster

34. Check the health of the Ceph cluster and its detailed status:
    
    sudo ceph health
    sudo ceph health detail
    sudo ceph -s
    

35. Create a Ceph storage pool:
    
    sudo ceph osd pool create rbd 200 3 (6 osds and 3 replica(2 hosts hence throws error) and default 100 pgs : 6*100 / 3 = 200 )
    

## Ceph Client Configuration

36. On the client, create a new block device, disable features that are not compatible with the kernel RBD driver, and map the block device:
    
    rbd create disk01 --size 4096
    rbd ls -l
    modprobe rbd
    rbd feature disable disk01 exclusive-lock object-map fast-diff deep-flatten
    rbd map disk01
    rbd showmapped
    

37. Create a filesystem on the new block device, create a mount point, and mount the block device:
    
    mkfs.xfs /dev/rbd0
    mkdir -p /mnt/mydisk
    mount /dev/rbd0 /mnt/mydisk
    df -h

38. Go to the controller:
sudo ceph osd tree
sudo ceph -s
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Commands to add new node in existing cluster 

On the compute 2 (add 3 more disks ) total 4
[root@compute2 ~]# history
    1  ifup ens33
    2  ip a
    3  reboot
    4  hostnamectl set-hostname compute3
    5  bash
    6  hostnamectl set-hostname compute2
    7  bash
    8   systemctl stop firewalld && systemctl disable firewalld
    9  yum install chrony -y
   10  useradd cephadm && echo "cdac" | passwd --stdin cephadm
   11  echo "cephadm ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephadm
   12   chmod 0440 /etc/sudoers.d/cephadm
   13   sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
   14  yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
   15  vi /etc/chrony.conf
   16  cat /etc/hosts
   17   systemctl restart chronyd
   18  chronyc sourcestats
   19  cat /etc/passwd
   20  lsblk
   21  history
[root@compute2 ~]#
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
on compute0
         lsblk
   36  vi /etc/hosts
   37  scp /etc/hosts root@compute1.hpcsa.com:/etc/hosts
   38  scp /etc/hosts root@monitor.hpcsa.com:/etc/hosts
   39  scp /etc/hosts root@client.hpcsa.com:/etc/hosts
   40  scp /etc/hosts root@controller.hpcsa.com:/etc/hosts
   41  cat /etc/passwd
   42  cat /etc/hosts
   43  scp /etc/hosts root@compute2.hpcsa.com:/etc/hosts
   44  history
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
run of controller
  40  ssh-copy-id cephadm@compute2
   41   vi ~/.ssh/config
   42   ssh cephadm@compute2
   43  cd ceph_cluster/
   44  ceph-deploy compute2
   45  ceph-deploy admin compute2
   46  ceph-deploy install compute2
   47  sudo ceph --version
   48  ceph-deploy admin compute2
   49  ceph-deploy mgr create compute2
   50  ceph-deploy disk list compute2
   51  ceph-deploy osd create --data /dev/sdb compute2
   52  ceph-deploy osd create --data /dev/sdc compute2
   53  ceph-deploy osd create --data /dev/sdd compute2
--------------------------------------------------------
   54   sudo ceph health
   55  sudo ceph health
   56  sudo ceph health detail
   57  sudo ceph -s
   58  sudo ceph osd tree
   59  sudo ceph health detail
   60  sudo ceph health
   61  sudo ceph osd pool ls detail
   62  sudo ceph health
   63  sudo ceph osd pool ls detail
   64  sudo ceph health detail
   66  sudo ceph -s
   75  sudo ceph health detail
   77  sudo ceph osd tree
   78  sudo ceph osd pool application enable rbd ceph
   79  sudo ceph health detail
   80  sudo ceph -s
   84  sudo ceph pg dump
   sudo ceph osd pool ls detail
   85  history
[cephadm@controller ceph_cluster]$
